{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d1e911",
   "metadata": {},
   "source": [
    "Loading LLM model: GPT 2 - training knowledge cutoff - 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f7118f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(2025)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f49a9b",
   "metadata": {},
   "source": [
    "Load processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeefab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 426 chunks\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"chunks.pkl\", \"rb\") as f:\n",
    "    all_chunks = pickle.load(f)\n",
    "print(f\"Loaded {len(all_chunks)} chunks\")  # e.g., 30 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b9520",
   "metadata": {},
   "source": [
    "Embed the chunks above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7cb03ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (426, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')  # From Day 1\n",
    "embeddings = embed_model.encode(all_chunks)  # List of arrays -> one big array\n",
    "embeddings = np.array(embeddings).astype('float32')  # FAISS needs this\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")  # e.g., (30, 384) – chunks x dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8feb1",
   "metadata": {},
   "source": [
    "Using Faiss for fast vector database searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e2a4863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built with 426 vectors\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "dimension = embeddings.shape[1]  # e.g., 384\n",
    "index = faiss.IndexFlatL2(dimension)  # Basic flat index (exact search, good for small data)\n",
    "index.add(embeddings)  # Train/add your vectors #type: Ignore\n",
    "print(f\"Index built with {index.ntotal} vectors\")  # Matches chunk count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "577a84f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test code \n",
    "# dummy_query = embed_model.encode([\"AI bias in ethics\"])\n",
    "# distances, indices = index.search(dummy_query, k=3)  # Top 3 nearest\n",
    "# print(\"Top indices:\", indices)  # e.g., [5, 12, 3] – chunk IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "489984f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing retrival functionality\n",
    "# from retrieval import *\n",
    "# sample_query = \"With all the information given, The name of the 46th president of the United States is\"\n",
    "# results = retrieve_chunks(sample_query, embed_model, index, all_chunks)\n",
    "# for chunk, score in results:\n",
    "#     print(f\"Score: {score:.2f} | Chunk: {chunk[:100]}...\")  # Preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc42638",
   "metadata": {},
   "source": [
    "Defining RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afd8a85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: The name of the 46th president of the United State (USA) is  | Plain: 9.61s | RAG: 9.74s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: Biden was born in  | Plain: 9.70s | RAG: 10.66s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: Biden studied at  | Plain: 9.45s | RAG: 9.50s\n",
      "Results saved to comparison_results.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "query",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "plain_answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rag_answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "plain_latency",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rag_latency",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "b23e6cfc-4bd0-4482-a697-7d175f12b817",
       "rows": [
        [
         "0",
         "The name of the 46th president of the United State (USA) is ",
         "Question: The name of the 46th president of the United State (USA) is \nAnswer: The name of the 46th president of the United States is \nQuestion: The name of the 46th president of the United States is \nAnswer: The name of the 46th president of the United States is \nQuestion: The name of the 46th president of the United States is \nQuestion: The name of the 46th president of the United States is \nQuestion: The name of the 46th president of the United States is \nQuestion: The name of the 46th president of the United States is \nQuestion: The name of the 46th president of the United States is \nQuestion: The name of the 46th president of the United States is \nQuestion: The name of the 46th president of the United States is \nQuestion: The name of the 46th president of the United States is \nQuestion: The name of the 46th president of the United States is \nQuestion: The name of the 46th president of the United States is \nQuestion: The name of the 46th president of the United States is \nQuestion: The name of the 46th president of the United States is \nQuestion: The name of the 46th president of the United States is \nQuestion:",
         "Use the following context to answer the question factually. If the context doesn't cover it, say \"I don't have info on that.\"\n    I know that: On January 7, Trump acknowledged the incoming administration without mentioning Biden's name.[333] Presidency (2021–2025) Inauguration Photo of Biden raising his right hand, with his left hand placed on a thick Bible Biden takes the oath of office administered by Chief Justice John Glover Roberts Jr. at the Capitol, January 20, 2021. Biden was inaugurated as the 46th president of the United States on January 20, 2021.[334][335] At 78, he was then the oldest person to assume the office (Trump\n\nequitable, and just. Joe Biden's path to the U.S. presidencyLearn more about the life and career of Joe Biden, the 46th president of the United States. See all videos for this article During the first weeks of his presidency, Biden signed a raft of executive orders, actions, and memoranda, many of which rescinded policies of the Trump administration, particularly in the areas of immigration, health care, and the environment. Notably, on his first day in office, Biden issued executive orders\n\npeople in four years than most Presidents have in eight. The 46th President of the United States Four years ago, we stood in a winter of peril and a winter of possibilities. We were in the grip of the worst pandemic in a century, the worst economic crisis since the Great Depression, and the worst attack on our democracy since the Civil War. But we came together as Americans, and we got through it. We emerged stronger, more prosperous, and more secure. Today, we have the strongest economy in the\n    With all the context above, I can answer that The name of the 46th president of the United State (USA) is  the oldest person to be elected president of the United States) and the youngest person to be elected president of the United States.\n\nBiden's presidency was marked by a series of scandals, including the impeachment of President Richard Nixon, the impeachment of President Bill Clinton, and the impeachment of President George W. Bush.\n\nBiden's presidency was marked by a series of scandals, including the impeachment of President Richard Nixon, the impeachment of President Bill Clinton, and the impeachment of President George W. Bush. In the first year of his presidency, Biden was the first African-American president to be elected to the United States Senate. He was elected to the Senate in 1864, and was confirmed in 1867.\n\nBiden's presidency was marked by a series of scandals, including the impeachment of President Richard Nixon, the impeachment of President Bill Clinton, and the impeachment of President George W. Bush. In the first year of his presidency, Biden was the first African-American president to be elected to the United States Senate. He was elected to the Senate in 1864, and was confirmed in 1867. In the first year of his presidency, Biden was the first African-American president to be elected to the United States Senate. He was elected to the",
         "9.609019041061401",
         "9.744885921478271"
        ],
        [
         "1",
         "Biden was born in ",
         "Question: Biden was born in \nAnswer: The first lady was born in the United States.\nQuestion: Biden was born in the United States.\nAnswer: The first lady was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion: Biden was born in the United States.\nQuestion",
         "Use the following context to answer the question factually. If the context doesn't cover it, say \"I don't have info on that.\"\n    I know that: Jr. was born on November 20, 1942, in Scranton, Pennsylvania. In 1953, the Biden family moved from Pennsylvania to Delaware. Joe Biden graduated from the University of Delaware and Syracuse Law School and served on the New Castle County Council. In 1972, at the age of 29, he was elected to the United States Senate. Just weeks after the election, tragedy struck the Biden family. Biden's wife, Neilia, and their one-year-old daughter, Naomi, were killed and their two young sons, Joseph “Beau” III\n\nhe was class president in his junior and senior years.[18][19] He graduated in 1961.[18] At the University of Delaware in Newark, Biden briefly played freshman football[20][21] and received a Bachelor of Arts degree with a double major in history and political science in 1965.[22][23] Marriages, law school, and early career (1966–1973) Biden married Neilia Hunter, a student at Syracuse University, on August 27, 1966,[22][24] after overcoming her parents' disinclination for her to wed a\n\nthe party's presumptive nominee in the 2024 presidential election. After his performance in the first presidential debate, renewed scrutiny from across the political spectrum about his cognitive ability led him to withdraw his candidacy. Biden's administration is ranked favorably by historians and scholars, diverging from public assessments of his tenure. He is the oldest living former U.S. president. Biography Joseph Robinette Biden, Jr. was born on November 20, 1942, in Scranton,\n    With all the context above, I can answer that Biden was born in  and Joseph “Drew, were killed in a car accident. The Biden family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The family was devastated. The",
         "9.704808235168457",
         "10.656056880950928"
        ],
        [
         "2",
         "Biden studied at ",
         "Question: Biden studied at \nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer: He studied at Harvard.\nAnswer:",
         "Use the following context to answer the question factually. If the context doesn't cover it, say \"I don't have info on that.\"\n    I know that: he was class president in his junior and senior years.[18][19] He graduated in 1961.[18] At the University of Delaware in Newark, Biden briefly played freshman football[20][21] and received a Bachelor of Arts degree with a double major in history and political science in 1965.[22][23] Marriages, law school, and early career (1966–1973) Biden married Neilia Hunter, a student at Syracuse University, on August 27, 1966,[22][24] after overcoming her parents' disinclination for her to wed a\n\ntreatment options are being reviewed.[197] It was also found that the cancer now spread from his prostate to other tissues in his body.[198] In September 2025, Biden's spokesperson said that he had recently had surgery to remove skin cancer lesions from his forehead.[199] Vice President Biden visiting Kosovo, May 2009 Biden has received honorary degrees from the University of Scranton (1976),[200] Saint Joseph's University (1981),[201] Widener University School of Law (2000),[202] Emerson\n\nclaims about his early life: that he had earned three degrees in college, that he attended law school on a full scholarship, that he had graduated in the top half of his class,[140][141] and that he had marched in the civil rights movement.[142] The limited amount of other news about the presidential race amplified these disclosures,[143] and on September 23, 1987, Biden withdrew his candidacy.[144] 2008 campaign Photo of Biden, casually dressed, talking with a citizen in a garden Biden\n    With all the context above, I can answer that Biden studied at  man who was a member of the Ku Klux Klan.[25]\n\nBiden's first wife, Mary Ann, was a member of the Ku Klux Klan. She was a member of the National Organization for Women, a group that was formed in the early 1960s to oppose the Vietnam War. She was a member of the National Organization for Women's National Committee, which was formed in the early 1960s to oppose the Vietnam War. She was a member of the National Organization for Women's National Committee, which was formed in the early 1960s to oppose the Vietnam War. She was a member of the National Organization for Women's National Committee, which was formed in the early 1960s to oppose the Vietnam War. She was a member of the National Organization for Women's National Committee, which was formed in the early 1960s to oppose the Vietnam War. She was a member of the National Organization for Women's National Committee, which was formed in the early 1960s to oppose the Vietnam War. She was a member of the National Organization for Women's National Committee, which was formed in the early 1960s to oppose the Vietnam War. She was a member of the National Organization for Women's National Committee, which was formed in the early 1960s to oppose the Vietnam War.",
         "9.447450160980225",
         "9.503504991531372"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>plain_answer</th>\n",
       "      <th>rag_answer</th>\n",
       "      <th>plain_latency</th>\n",
       "      <th>rag_latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The name of the 46th president of the United S...</td>\n",
       "      <td>Question: The name of the 46th president of th...</td>\n",
       "      <td>Use the following context to answer the questi...</td>\n",
       "      <td>9.609019</td>\n",
       "      <td>9.744886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Biden was born in</td>\n",
       "      <td>Question: Biden was born in \\nAnswer: The firs...</td>\n",
       "      <td>Use the following context to answer the questi...</td>\n",
       "      <td>9.704808</td>\n",
       "      <td>10.656057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Biden studied at</td>\n",
       "      <td>Question: Biden studied at \\nAnswer: He studie...</td>\n",
       "      <td>Use the following context to answer the questi...</td>\n",
       "      <td>9.447450</td>\n",
       "      <td>9.503505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  The name of the 46th president of the United S...   \n",
       "1                                 Biden was born in    \n",
       "2                                  Biden studied at    \n",
       "\n",
       "                                        plain_answer  \\\n",
       "0  Question: The name of the 46th president of th...   \n",
       "1  Question: Biden was born in \\nAnswer: The firs...   \n",
       "2  Question: Biden studied at \\nAnswer: He studie...   \n",
       "\n",
       "                                          rag_answer  plain_latency  \\\n",
       "0  Use the following context to answer the questi...       9.609019   \n",
       "1  Use the following context to answer the questi...       9.704808   \n",
       "2  Use the following context to answer the questi...       9.447450   \n",
       "\n",
       "   rag_latency  \n",
       "0     9.744886  \n",
       "1    10.656057  \n",
       "2     9.503505  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from pipelines import *\n",
    "\n",
    "def compare_responses(queries, embed_model, index, all_chunks, max_length=100):\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Plain LLM\n",
    "        plain_prompt = f\"Question: {query}\\nAnswer:\"\n",
    "        plain_answer = generate_response(generator, plain_prompt, max_length) \n",
    "        plain_time = time.time() - start_time\n",
    "        \n",
    "        # RAG\n",
    "        rag_start = time.time()\n",
    "        rag_answer = rag_pipeline(generator, query, embed_model, index, all_chunks, max_length) \n",
    "        rag_time = time.time() - rag_start\n",
    "        \n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'plain_answer': plain_answer[0][\"generated_text\"],\n",
    "            'rag_answer': rag_answer[0][\"generated_text\"],\n",
    "            'plain_latency': plain_time,\n",
    "            'rag_latency': rag_time\n",
    "        })\n",
    "        print(f\"Processed: {query} | Plain: {plain_time:.2f}s | RAG: {rag_time:.2f}s\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('comparison_results.csv', index=False)\n",
    "    print(\"Results saved to comparison_results.csv\")\n",
    "    return df\n",
    "\n",
    "# Test with 3 quick queries first\n",
    "test_queries = [\"The name of the 46th president of the United State (USA) is \", \"Biden was born in \", \"Biden studied at \"]\n",
    "df = compare_responses(test_queries, embed_model, index, all_chunks, max_length=100)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
