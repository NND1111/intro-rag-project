{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b193a7",
   "metadata": {},
   "source": [
    "Ingestion - Input data and chunk them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed article_ag as source_id 1, added 21 chunks\n",
      "Processed article_sa as source_id 2, added 7 chunks\n",
      "Processed article_th as source_id 3, added 88 chunks\n",
      "Processed diggit_mag as source_id 4, added 32 chunks\n",
      "Processed editorial_ as source_id 5, added 19 chunks\n",
      "Processed fullbright as source_id 6, added 6 chunks\n",
      "Processed In_search_ as source_id 7, added 46 chunks\n",
      "Processed reuters_th as source_id 8, added 7 chunks\n",
      "Processed saigoneer. as source_id 9, added 17 chunks\n",
      "Processed song_your_ as source_id 10, added 3 chunks\n",
      "Processed TCS-Buddhi as source_id 11, added 198 chunks\n",
      "Processed thenewlens as source_id 12, added 10 chunks\n",
      "Processed the_new_yo as source_id 13, added 11 chunks\n",
      "Processed Trinh Cong as source_id 14, added 19 chunks\n",
      "Processed Trinh_Cong as source_id 15, added 9 chunks\n",
      "Processed trinh_cong as source_id 16, added 3 chunks\n",
      "Processed trinh_cong as source_id 17, added 3 chunks\n",
      "Processed Vietcetera as source_id 18, added 18 chunks\n",
      "Processed vietcetera as source_id 19, added 17 chunks\n",
      "Processed wikipedia_ as source_id 20, added 40 chunks\n",
      "Total chunks created: 574\n",
      "Data retrieval is complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "import pickle  \n",
    "\n",
    "# Path to your data folder\n",
    "data_dir = \"pre_chosen_sources\"  \n",
    "# List to hold all chunks\n",
    "all_chunks = []\n",
    "source_map = {}\n",
    "\n",
    "#Getting all files\n",
    "files = [f for f in os.listdir(data_dir) if f.lower().endswith(('.txt', '.pdf'))] # only support text file and pdf file for data extraction\n",
    "\n",
    "for id, file_name in enumerate(files, start=1):\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    source_map[id] = file_name\n",
    "\n",
    "    #Load according to the file extension\n",
    "    if file_name.lower().endswith(\".txt\"):\n",
    "        loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "    elif file_name.lower().endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    else:\n",
    "        continue # skip loop if file is not supported\n",
    "\n",
    "    docs = loader.load()\n",
    "    text = \" \".join([doc.page_content for doc in docs])\n",
    "    text = ' '.join(text.split())  # Collapses multiples\n",
    "    text = text.replace('\\n', ' ')  # Flatten newlines if any\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,  # ~500 chars/tokens\n",
    "            chunk_overlap=50  # Overlap for context continuity\n",
    "\n",
    "        )\n",
    "    file_chunks = splitter.split_text(text)\n",
    "    for chunk in file_chunks:\n",
    "        all_chunks.append({'text': chunk, 'source_id': id}) #updated to cite according to sources' name\n",
    "    \n",
    "    print(f\"Processed {file_name[:10]} ... as source_id {id}, added {len(file_chunks)} chunks\")\n",
    "\n",
    "\n",
    "# Final count\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "\n",
    "# Save data \n",
    "with open(\"chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_chunks, f)\n",
    "with open(\"source_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(source_map, f)\n",
    "\n",
    "print(\"Data retrieval is complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1e911",
   "metadata": {},
   "source": [
    "Huggingface.co: gpt-oss-20b — for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f7118f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline, set_seed\n",
    "\n",
    "# generator = pipeline('question-answering', model='gpt2')\n",
    "# set_seed(2025)\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "client = InferenceClient(model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f49a9b",
   "metadata": {},
   "source": [
    "Load processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aeefab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 574 chunks\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"chunks.pkl\", \"rb\") as f:\n",
    "    all_chunks = pickle.load(f)\n",
    "print(f\"Loaded {len(all_chunks)} chunks\")  # e.g., 30 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b9520",
   "metadata": {},
   "source": [
    "Embed the chunks above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7cb03ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (574, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "embeddings = embed_model.encode(all_chunks)  #\n",
    "embeddings = np.array(embeddings).astype('float32')  # FAISS needs this\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")  # e.g., (30, 384) – chunks x dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8feb1",
   "metadata": {},
   "source": [
    "Using Faiss for fast vector database searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e2a4863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built with 574 vectors\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "dimension = embeddings.shape[1]  # e.g., 384\n",
    "index = faiss.IndexFlatL2(dimension)  # Basic flat index (exact search, good for small data)\n",
    "index.add(embeddings)  # Train/add your vectors\n",
    "print(f\"Index built with {index.ntotal} vectors\")  # Matches chunk count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc42638",
   "metadata": {},
   "source": [
    "Chat completion and comparision between two pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd8a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from pipelines import * \n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pickle  \n",
    "\n",
    "\n",
    "def augment_prompt_no_query(retrieved_chunks):\n",
    "    context_parts = []\n",
    "    for chunk_tuple in retrieved_chunks:\n",
    "        chunk_dict = chunk_tuple[0]  # {'text':, 'source_id':}\n",
    "        source_id = chunk_dict['source_id']\n",
    "        text = chunk_dict['text']\n",
    "        context_parts.append(f\"[Source ID: {source_id}]\\n{text}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    prompt = f\"\"\"Use the following context to answer the question factually. If the context doesn't cover it, say \"I don't have info on that. You MUST always provide a response—do not return nothing or an empty answer\"\n",
    "    \n",
    "            Context: {context}\"\"\"\n",
    "    return prompt  # this is prompt engineering\n",
    "\n",
    " # Use embedding model for Post-Generation Similarity Matching in conjunction with traditional regex searching\n",
    "def attribute_sources(ragged_output, retrieved_chunks, embed_model, threshold=0.7):\n",
    "    if not ragged_output:\n",
    "        return []\n",
    "    \n",
    "\n",
    "    sentences = [s.strip() for s in ragged_output.split('.') if s.strip()]\n",
    "    \n",
    "    if not sentences:\n",
    "        return []\n",
    "    \n",
    "    # Embed response sentences\n",
    "    sent_embeds = embed_model.encode(sentences)\n",
    "    \n",
    "    # Embed retrieved chunks\n",
    "    chunk_texts = [chunk_tuple[0]['text'] for chunk_tuple in retrieved_chunks]\n",
    "    chunk_embeds = embed_model.encode(chunk_texts)\n",
    "    \n",
    "    used_source_ids = set()\n",
    "    for sent_embed in sent_embeds:\n",
    "        similarities = util.cos_sim(sent_embed, chunk_embeds)[0]\n",
    "        max_sim_idx = np.argmax(similarities)\n",
    "        if similarities[max_sim_idx] >= threshold:\n",
    "            source_id = retrieved_chunks[max_sim_idx][0]['source_id']\n",
    "            used_source_ids.add(source_id)\n",
    "    \n",
    "    return sorted(used_source_ids)\n",
    "\n",
    "def compare_ragged_outputs(queries, embed_model, index, all_chunks, max_length=300):\n",
    "    with open(\"source_map.pkl\", \"rb\") as f:\n",
    "        SOURCE_MAP = pickle.load(f)\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Plain LLM\n",
    "        plain_messages = [{\"role\": \"system\", \"content\": \"You MUST always provide a response—do not return nothing or an empty answer\"}, {\"role\": \"user\", \"content\": query}]\n",
    "        plain_response = client.chat_completion(plain_messages, max_tokens=300)\n",
    "        plain_output = plain_response.choices[0].message.content  # Extract content\n",
    "        plain_time = time.time() - start_time\n",
    "\n",
    "        if plain_output is None: # can still return None if there is error in API call\n",
    "            plain_output = \"\"  # Treat as empty string\n",
    "            print(f\"Warning: LLM returned None for query '{query}'—handling as empty.\")\n",
    "\n",
    "        # RAG\n",
    "        rag_start = time.time()\n",
    "        retrieved = retrieve_chunks(query, embed_model, index, all_chunks)\n",
    "        rag_messages = [{\"role\": \"system\", \"content\": augment_prompt_no_query(retrieved)}, {\"role\": \"user\", \"content\": query}]  # System first\n",
    "        rag_response = client.chat_completion(rag_messages, max_tokens=300)\n",
    "        ragged_output = rag_response.choices[0].message.content\n",
    "        used_sources = set()\n",
    "        \n",
    "        if ragged_output is None:\n",
    "            ragged_output = \"\"  # Treat as empty string\n",
    "            print(f\"Warning: RAG model returned None for query '{query}'—handling as empty.\")\n",
    "\n",
    "        # Step 1: Try regex parsing for inline citations (as fallback or hybrid)\n",
    "        used_source_ids_regex = set(re.findall(r'\\[Source ID: \\d+\\]', ragged_output))  # Fixed regex to match [Source ID: x]\n",
    "        \n",
    "        # Step 2: Use similarity matching as primary method\n",
    "        used_source_ids_sim = set(attribute_sources(ragged_output, retrieved, embed_model))\n",
    "        \n",
    "        # Combine: Use similarity primarily, add regex if any unique\n",
    "        used_source_ids = used_source_ids_sim.union(used_source_ids_regex)\n",
    "        \n",
    "        # Map to file names\n",
    "        used_sources = {SOURCE_MAP[int(sid)] for sid in used_source_ids if int(sid) in SOURCE_MAP}\n",
    "        \n",
    "        # If no citations found, fall back to all retrieved sources\n",
    "        if not used_sources:\n",
    "            used_source_ids_all = set(chunk_tuple[0]['source_id'] for chunk_tuple in retrieved)\n",
    "            used_sources = {SOURCE_MAP[sid] for sid in used_source_ids_all if sid in SOURCE_MAP}\n",
    "        \n",
    "        # Sort and format\n",
    "        sources_list = sorted(used_sources)\n",
    "        sources_str = \"Sources: \" + \", \".join(sources_list) if sources_list else \"No sources used.\"\n",
    "        \n",
    "        # Append to ragged_output\n",
    "        full_output = ragged_output + \"\\n\\n\" + sources_str if ragged_output else \"No output generated.\"\n",
    "\n",
    "        rag_time = time.time() - rag_start\n",
    "        \n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'plain_answer': plain_output,\n",
    "            'rag_answer': full_output,\n",
    "            'plain_latency': plain_time,\n",
    "            'rag_latency': rag_time\n",
    "        })\n",
    "        print(f\"Processed: {query} | Plain: {plain_time:.2f}s | RAG: {rag_time:.2f}s\")\n",
    "    \n",
    "    # Save to CSV if you want\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('comparison_results.csv', index=False)\n",
    "    print(\"Results saved to comparison_results.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4706d9",
   "metadata": {},
   "source": [
    "TEST SPACE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ac757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "#uncomment to test\n",
    "# test_queries = [\n",
    "#         \"What was the first song made by Trinh Cong Son?\",\n",
    "#         \"Where was Trinh Cong Son born?\", \n",
    "#         \"How many songs did Trinh Cong Son produce?\",\n",
    "#         \"Trinh Cong Son was nickname?\", \n",
    "#         \"Trinh Cong Son favorite song is ?\", \n",
    "#         \"How influential was Trinh Cong Son's legacy\",\n",
    "#         \"Was Trinh Cong Son consider 'Bob Dylan 'of Vietnam\",\n",
    "#         \"DId Trinh Cong Son write antiwar songs?\"]\n",
    "# df = compare_ragged_outputs(test_queries, embed_model, index, all_chunks, max_length=100)\n",
    "# df\n",
    "#RAG shows no hallucination and cite its sources. Plain LLM hallucinates more. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
