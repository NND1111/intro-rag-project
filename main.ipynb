{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b193a7",
   "metadata": {},
   "source": [
    "Ingestion - Input data and chunk them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7d5815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\\1.txt...\n",
      "Added 238 chunks from 1.txt\n",
      "Loading data\\2.txt...\n",
      "Added 11 chunks from 2.txt\n",
      "Loading data\\3.txt...\n",
      "Added 60 chunks from 3.txt\n",
      "Loading data\\4.txt...\n",
      "Added 63 chunks from 4.txt\n",
      "Loading data\\5.txt...\n",
      "Added 13 chunks from 5.txt\n",
      "Loading data\\6.txt...\n",
      "Added 6 chunks from 6.txt\n",
      "Loading data\\7.txt...\n",
      "Added 8 chunks from 7.txt\n",
      "Loading data\\8.txt...\n",
      "Added 9 chunks from 8.txt\n",
      "Loading data\\9.txt...\n",
      "Added 5 chunks from 9.txt\n",
      "Loading data\\10.txt...\n",
      "Added 13 chunks from 10.txt\n",
      "Total chunks created: 426\n",
      "Chunks saved to chunks.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import pickle  # For saving chunks if needed\n",
    "\n",
    "# Path to your data folder\n",
    "data_dir = \"data\"  # Assumes /data in your repo root\n",
    "\n",
    "# List to hold all chunks\n",
    "all_chunks = []\n",
    "\n",
    "# Loop over files 1.txt to 15.txt (skips missing ones)\n",
    "for i in range(1, 11):  # 1 to 15\n",
    "    file_path = os.path.join(data_dir, f\"{i}.txt\")\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading {file_path}...\")\n",
    "        loader = TextLoader(file_path, encoding=\"utf-8\")  # Handles standard text\n",
    "        docs = loader.load()\n",
    "        text = \" \".join([doc.page_content for doc in docs])  # Just the text\n",
    "\n",
    "        # Clean: Remove extra whitespace, newlines (basic)\n",
    "        text = ' '.join(text.split())  # Collapses multiples\n",
    "        text = text.replace('\\n', ' ')  # Flatten newlines if any\n",
    "\n",
    "        # Chunk this file's text\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,  # ~500 chars/tokens\n",
    "            chunk_overlap=50  # Overlap for context continuity\n",
    "\n",
    "        )\n",
    "        file_chunks = splitter.split_text(text)\n",
    "        for chunk in file_chunks:\n",
    "            all_chunks.append({'text': chunk, 'source': f\"{i}.txt\"})\n",
    "        print(f\"Added {len(file_chunks)} chunks from {i}.txt\")\n",
    "    else:\n",
    "        print(f\"Skipping {file_path} (not found)\")\n",
    "\n",
    "# Final count\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "\n",
    "\n",
    "with open(\"chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_chunks, f)\n",
    "print(\"Chunks saved to chunks.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1e911",
   "metadata": {},
   "source": [
    "Huggingface.co: gpt-oss-20b — for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7118f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline, set_seed\n",
    "\n",
    "# generator = pipeline('question-answering', model='gpt2')\n",
    "# set_seed(2025)\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "client = InferenceClient(model=\"openai/gpt-oss-20b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f49a9b",
   "metadata": {},
   "source": [
    "Load processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeefab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 426 chunks\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"chunks.pkl\", \"rb\") as f:\n",
    "    all_chunks = pickle.load(f)\n",
    "print(f\"Loaded {len(all_chunks)} chunks\")  # e.g., 30 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b9520",
   "metadata": {},
   "source": [
    "Embed the chunks above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb949ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7cb03ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (426, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')  # From Day 1\n",
    "embeddings = embed_model.encode(all_chunks)  # List of arrays -> one big array\n",
    "embeddings = np.array(embeddings).astype('float32')  # FAISS needs this\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")  # e.g., (30, 384) – chunks x dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8feb1",
   "metadata": {},
   "source": [
    "Using Faiss for fast vector database searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e2a4863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built with 426 vectors\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "dimension = embeddings.shape[1]  # e.g., 384\n",
    "index = faiss.IndexFlatL2(dimension)  # Basic flat index (exact search, good for small data)\n",
    "index.add(embeddings)  # Train/add your vectors #type: Ignore\n",
    "print(f\"Index built with {index.ntotal} vectors\")  # Matches chunk count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc42638",
   "metadata": {},
   "source": [
    "Chat completion and comparision between two pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afd8a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting response\n",
    "import pandas as pd\n",
    "import time\n",
    "from pipelines import *  # Assuming this imports retrieve_chunks, etc.\n",
    "import re\n",
    "from huggingface_hub import InferenceClient  # Add if not in pipelines\n",
    "\n",
    "# Assume client is defined, e.g.\n",
    "\n",
    "\n",
    "# Define augment_prompt_no_query based on the idea\n",
    "def augment_prompt_no_query(retrieved_chunks):\n",
    "    context_parts = []\n",
    "    for chunk_tuple in retrieved_chunks:\n",
    "        chunk_dict = chunk_tuple[0]  # {'text':, 'source':}\n",
    "        source = chunk_dict['source']\n",
    "        text = chunk_dict['text']\n",
    "        context_parts.append(f\"[Source: {source}]\\n{text}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    prompt = f\"\"\"Use the following context to answer the question factually. If the context doesn't cover it, say \"I don't have info on that.\"\n",
    "When referencing information from the context, include the [Source: x.txt] inline in your answer where it's used.\n",
    "    \n",
    "Context: {context}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def compare_ragged_outputs(queries, embed_model, index, all_chunks, max_length=300):\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Plain LLM\n",
    "        plain_messages = [{\"role\": \"user\", \"content\": query}]\n",
    "        plain_response = client.chat_completion(plain_messages, max_tokens=300)\n",
    "        plain_output = plain_response.choices[0].message.content  # Extract content\n",
    "        plain_time = time.time() - start_time\n",
    "        \n",
    "        # RAG\n",
    "        rag_start = time.time()\n",
    "        retrieved = retrieve_chunks(query, embed_model, index, all_chunks)\n",
    "        rag_messages = [{\"role\": \"system\", \"content\": augment_prompt_no_query(retrieved)}, {\"role\": \"user\", \"content\": query}]  # System first\n",
    "        rag_response = client.chat_completion(rag_messages, max_tokens=300)\n",
    "        ragged_output = rag_response.choices[0].message.content\n",
    "        used_sources = \"\"\n",
    "        if ragged_output:\n",
    "            used_sources = set(re.findall(pattern=r'\\[Source: \\(\\d+\\.txt\\)\\]', string=ragged_output))\n",
    "        \n",
    "        # If no citations parsed, fall back to all retrieved sources\n",
    "        if not used_sources:\n",
    "            used_sources = set(chunk_tuple[0]['source'] for chunk_tuple in retrieved)\n",
    "        \n",
    "        # Sort and format\n",
    "        sources_list = sorted(used_sources)\n",
    "        sources_str = \"Sources: \" + \", \".join(sources_list) if used_sources else \"No sources used.\"\n",
    "        \n",
    "        # Append to ragged_output\n",
    "        full_output = ragged_output + \"\\n\\n\" + sources_str\n",
    "\n",
    "        rag_time = time.time() - rag_start\n",
    "        \n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'plain_answer': plain_output,\n",
    "            'rag_answer': full_output,\n",
    "            'plain_latency': plain_time,\n",
    "            'rag_latency': rag_time\n",
    "        })\n",
    "        print(f\"Processed: {query} | Plain: {plain_time:.2f}s | RAG: {rag_time:.2f}s\")\n",
    "    \n",
    "    # Save to CSV if you want\n",
    "    df = pd.DataFrame(results)\n",
    "    # df.to_csv('comparison_results.csv', index=False)\n",
    "    # print(\"Results saved to comparison_results.csv\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b93ac757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: Tell me what happened to Joe Biden's , the 46th president of the USA, wife | Plain: 3.16s | RAG: 1.20s\n",
      "Processed: Tell me Where was Joe Biden, the 46th president of the USA, born | Plain: 1.04s | RAG: 1.22s\n",
      "Processed: Tell me where did Joe Biden, the 46th president of the USA, graduated | Plain: 2.06s | RAG: 2.28s\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "query",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "plain_answer",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "rag_answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "plain_latency",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rag_latency",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "5145dc92-a2d1-4d61-849a-0dd82f35d7ae",
       "rows": [
        [
         "0",
         "Tell me what happened to Joe Biden's , the 46th president of the USA, wife",
         "Jill Biden, the wife of President Joe Biden (the 46th president of the United States), is alive and continues to play an active public role.  She has served as the First Lady since **",
         "I don't have info on that.\n\nSources: 1.txt",
         "3.160740375518799",
         "1.2013530731201172"
        ],
        [
         "1",
         "Tell me Where was Joe Biden, the 46th president of the USA, born",
         "Joe Biden, the 46th President of the United States, was born in **Scranton, Pennsylvania**.",
         "Joe Biden was born in Scranton, Pennsylvania. [Source: 1.txt]\n\nSources: 1.txt",
         "1.0379421710968018",
         "1.2150473594665527"
        ],
        [
         "2",
         "Tell me where did Joe Biden, the 46th president of the USA, graduated",
         null,
         "Joe Biden earned his undergraduate degree from the **University of Delaware** in 1965 and went on to law school at **Syracuse University College of Law**, graduating in 1968. [Source: 1.txt]\n\nSources: 1.txt",
         "2.05607271194458",
         "2.2762274742126465"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>plain_answer</th>\n",
       "      <th>rag_answer</th>\n",
       "      <th>plain_latency</th>\n",
       "      <th>rag_latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tell me what happened to Joe Biden's , the 46t...</td>\n",
       "      <td>Jill Biden, the wife of President Joe Biden (t...</td>\n",
       "      <td>I don't have info on that.\\n\\nSources: 1.txt</td>\n",
       "      <td>3.160740</td>\n",
       "      <td>1.201353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tell me Where was Joe Biden, the 46th presiden...</td>\n",
       "      <td>Joe Biden, the 46th President of the United St...</td>\n",
       "      <td>Joe Biden was born in Scranton, Pennsylvania. ...</td>\n",
       "      <td>1.037942</td>\n",
       "      <td>1.215047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tell me where did Joe Biden, the 46th presiden...</td>\n",
       "      <td>None</td>\n",
       "      <td>Joe Biden earned his undergraduate degree from...</td>\n",
       "      <td>2.056073</td>\n",
       "      <td>2.276227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  Tell me what happened to Joe Biden's , the 46t...   \n",
       "1  Tell me Where was Joe Biden, the 46th presiden...   \n",
       "2  Tell me where did Joe Biden, the 46th presiden...   \n",
       "\n",
       "                                        plain_answer  \\\n",
       "0  Jill Biden, the wife of President Joe Biden (t...   \n",
       "1  Joe Biden, the 46th President of the United St...   \n",
       "2                                               None   \n",
       "\n",
       "                                          rag_answer  plain_latency  \\\n",
       "0       I don't have info on that.\\n\\nSources: 1.txt       3.160740   \n",
       "1  Joe Biden was born in Scranton, Pennsylvania. ...       1.037942   \n",
       "2  Joe Biden earned his undergraduate degree from...       2.056073   \n",
       "\n",
       "   rag_latency  \n",
       "0     1.201353  \n",
       "1     1.215047  \n",
       "2     2.276227  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST\n",
    "# Test with 3 quick queries first\n",
    "test_queries = [\"Tell me what happened to Joe Biden's , the 46th president of the USA, wife\", \"Tell me Where was Joe Biden, the 46th president of the USA, born\", \"Tell me where did Joe Biden, the 46th president of the USA, graduated\"]\n",
    "df = compare_ragged_outputs(test_queries, embed_model, index, all_chunks, max_length=100)\n",
    "df\n",
    "#RAG shows no hallucination and cite its sources. Plain LLM hallucinates more. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
