{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b193a7",
   "metadata": {},
   "source": [
    "Ingestion - Input data and chunk them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7d5815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1.txt as source_id 1, added 238 chunks\n",
      "Processed 10.txt as source_id 2, added 13 chunks\n",
      "Processed 2.txt as source_id 3, added 11 chunks\n",
      "Processed 3.txt as source_id 4, added 60 chunks\n",
      "Processed 4.txt as source_id 5, added 63 chunks\n",
      "Processed 5.txt as source_id 6, added 13 chunks\n",
      "Processed 6.txt as source_id 7, added 6 chunks\n",
      "Processed 7.txt as source_id 8, added 8 chunks\n",
      "Processed 8.txt as source_id 9, added 9 chunks\n",
      "Processed 9.txt as source_id 10, added 5 chunks\n",
      "Total chunks created: 426\n",
      "Data retrieval is complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "import pickle  # For saving chunks if needed\n",
    "\n",
    "# Path to your data folder\n",
    "data_dir = \"data\"  # Assumes /data in your repo root\n",
    "\n",
    "# List to hold all chunks\n",
    "all_chunks = []\n",
    "source_map = {}\n",
    "\n",
    "#Getting all files\n",
    "files = [f for f in os.listdir(data_dir) if f.lower().endswith(('.txt', '.pdf'))] # only support text file and pdf file for data extraction\n",
    "\n",
    "for id, file_name in enumerate(files, start=1):\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    source_map[id] = file_name\n",
    "\n",
    "    #Load according to the file extension\n",
    "    if file_name.lower().endswith(\".txt\"):\n",
    "        loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "    elif file_name.lower().endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    else:\n",
    "        continue # skip loop if file is not supported\n",
    "\n",
    "    docs = loader.load()\n",
    "    text = \" \".join([doc.page_content for doc in docs])\n",
    "    text = ' '.join(text.split())  # Collapses multiples\n",
    "    text = text.replace('\\n', ' ')  # Flatten newlines if any\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,  # ~500 chars/tokens\n",
    "            chunk_overlap=50  # Overlap for context continuity\n",
    "\n",
    "        )\n",
    "    file_chunks = splitter.split_text(text)\n",
    "    for chunk in file_chunks:\n",
    "        all_chunks.append({'text': chunk, 'source_id': id}) #updated to cite according to sources' name\n",
    "    \n",
    "    print(f\"Processed {file_name[:10]} as source_id {id}, added {len(file_chunks)} chunks\")\n",
    "\n",
    "\n",
    "# Final count\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "\n",
    "# Save data \n",
    "with open(\"chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_chunks, f)\n",
    "with open(\"source_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(source_map, f)\n",
    "\n",
    "print(\"Data retrieval is complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1e911",
   "metadata": {},
   "source": [
    "Huggingface.co: gpt-oss-20b — for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7118f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline, set_seed\n",
    "\n",
    "# generator = pipeline('question-answering', model='gpt2')\n",
    "# set_seed(2025)\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "client = InferenceClient(model=\"openai/gpt-oss-20b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f49a9b",
   "metadata": {},
   "source": [
    "Load processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeefab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 426 chunks\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"chunks.pkl\", \"rb\") as f:\n",
    "    all_chunks = pickle.load(f)\n",
    "print(f\"Loaded {len(all_chunks)} chunks\")  # e.g., 30 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b9520",
   "metadata": {},
   "source": [
    "Embed the chunks above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb949ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7cb03ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (426, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "embeddings = embed_model.encode(all_chunks)  #\n",
    "embeddings = np.array(embeddings).astype('float32')  # FAISS needs this\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")  # e.g., (30, 384) – chunks x dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8feb1",
   "metadata": {},
   "source": [
    "Using Faiss for fast vector database searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e2a4863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built with 426 vectors\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "dimension = embeddings.shape[1]  # e.g., 384\n",
    "index = faiss.IndexFlatL2(dimension)  # Basic flat index (exact search, good for small data)\n",
    "index.add(embeddings)  # Train/add your vectors\n",
    "print(f\"Index built with {index.ntotal} vectors\")  # Matches chunk count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc42638",
   "metadata": {},
   "source": [
    "Chat completion and comparision between two pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd8a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from pipelines import * \n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pickle  \n",
    "\n",
    "\n",
    "def augment_prompt_no_query(retrieved_chunks):\n",
    "    context_parts = []\n",
    "    for chunk_tuple in retrieved_chunks:\n",
    "        chunk_dict = chunk_tuple[0]  # {'text':, 'source_id':}\n",
    "        source_id = chunk_dict['source_id']\n",
    "        text = chunk_dict['text']\n",
    "        context_parts.append(f\"[Source ID: {source_id}]\\n{text}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    prompt = f\"\"\"Use the following context to answer the question factually. If the context doesn't cover it, say \"I don't have info on that. You MUST always provide a response—do not return nothing or an empty answer\"\n",
    "    \n",
    "            Context: {context}\"\"\"\n",
    "    return prompt  # this is prompt engineering\n",
    "\n",
    " # Use embedding model for Post-Generation Similarity Matching in conjunction with traditional regex searching\n",
    "def attribute_sources(ragged_output, retrieved_chunks, embed_model, threshold=0.7):\n",
    "    if not ragged_output:\n",
    "        return []\n",
    "    \n",
    "\n",
    "    sentences = [s.strip() for s in ragged_output.split('.') if s.strip()]\n",
    "    \n",
    "    if not sentences:\n",
    "        return []\n",
    "    \n",
    "    # Embed response sentences\n",
    "    sent_embeds = embed_model.encode(sentences)\n",
    "    \n",
    "    # Embed retrieved chunks\n",
    "    chunk_texts = [chunk_tuple[0]['text'] for chunk_tuple in retrieved_chunks]\n",
    "    chunk_embeds = embed_model.encode(chunk_texts)\n",
    "    \n",
    "    used_source_ids = set()\n",
    "    for sent_embed in sent_embeds:\n",
    "        similarities = util.cos_sim(sent_embed, chunk_embeds)[0]\n",
    "        max_sim_idx = np.argmax(similarities)\n",
    "        if similarities[max_sim_idx] >= threshold:\n",
    "            source_id = retrieved_chunks[max_sim_idx][0]['source_id']\n",
    "            used_source_ids.add(source_id)\n",
    "    \n",
    "    return sorted(used_source_ids)\n",
    "\n",
    "def compare_ragged_outputs(queries, embed_model, index, all_chunks, max_length=300):\n",
    "    with open(\"source_map.pkl\", \"rb\") as f:\n",
    "        SOURCE_MAP = pickle.load(f)\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Plain LLM\n",
    "        plain_messages = [{\"role\": \"system\", \"content\": \"You MUST always provide a response—do not return nothing or an empty answer\"}, {\"role\": \"user\", \"content\": query}]\n",
    "        plain_response = client.chat_completion(plain_messages, max_tokens=300)\n",
    "        plain_output = plain_response.choices[0].message.content  # Extract content\n",
    "        plain_time = time.time() - start_time\n",
    "\n",
    "        if plain_output is None: # can still return None if there is error in API call\n",
    "            plain_output = \"\"  # Treat as empty string\n",
    "            print(f\"Warning: LLM returned None for query '{query}'—handling as empty.\")\n",
    "\n",
    "        # RAG\n",
    "        rag_start = time.time()\n",
    "        retrieved = retrieve_chunks(query, embed_model, index, all_chunks)\n",
    "        rag_messages = [{\"role\": \"system\", \"content\": augment_prompt_no_query(retrieved)}, {\"role\": \"user\", \"content\": query}]  # System first\n",
    "        rag_response = client.chat_completion(rag_messages, max_tokens=300)\n",
    "        ragged_output = rag_response.choices[0].message.content\n",
    "        used_sources = set()\n",
    "        \n",
    "        if ragged_output is None:\n",
    "            ragged_output = \"\"  # Treat as empty string\n",
    "            print(f\"Warning: RAG model returned None for query '{query}'—handling as empty.\")\n",
    "\n",
    "        # Step 1: Try regex parsing for inline citations (as fallback or hybrid)\n",
    "        used_source_ids_regex = set(re.findall(r'\\[Source ID: \\d+\\]', ragged_output))  # Fixed regex to match [Source ID: x]\n",
    "        \n",
    "        # Step 2: Use similarity matching as primary method\n",
    "        used_source_ids_sim = set(attribute_sources(ragged_output, retrieved, embed_model))\n",
    "        \n",
    "        # Combine: Use similarity primarily, add regex if any unique\n",
    "        used_source_ids = used_source_ids_sim.union(used_source_ids_regex)\n",
    "        \n",
    "        # Map to file names\n",
    "        used_sources = {SOURCE_MAP[int(sid)] for sid in used_source_ids if int(sid) in SOURCE_MAP}\n",
    "        \n",
    "        # If no citations found, fall back to all retrieved sources\n",
    "        if not used_sources:\n",
    "            used_source_ids_all = set(chunk_tuple[0]['source_id'] for chunk_tuple in retrieved)\n",
    "            used_sources = {SOURCE_MAP[sid] for sid in used_source_ids_all if sid in SOURCE_MAP}\n",
    "        \n",
    "        # Sort and format\n",
    "        sources_list = sorted(used_sources)\n",
    "        sources_str = \"Sources: \" + \", \".join(sources_list) if sources_list else \"No sources used.\"\n",
    "        \n",
    "        # Append to ragged_output\n",
    "        full_output = ragged_output + \"\\n\\n\" + sources_str if ragged_output else \"No output generated.\"\n",
    "\n",
    "        rag_time = time.time() - rag_start\n",
    "        \n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'plain_answer': plain_output,\n",
    "            'rag_answer': full_output,\n",
    "            'plain_latency': plain_time,\n",
    "            'rag_latency': rag_time\n",
    "        })\n",
    "        print(f\"Processed: {query} | Plain: {plain_time:.2f}s | RAG: {rag_time:.2f}s\")\n",
    "    \n",
    "    # Save to CSV if you want\n",
    "    df = pd.DataFrame(results)\n",
    "    # df.to_csv('comparison_results.csv', index=False)\n",
    "    # print(\"Results saved to comparison_results.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ac757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: What happened to Bidens' wife? | Plain: 2.85s | RAG: 1.67s\n",
      "Processed: Where was Joe Biden born? | Plain: 1.00s | RAG: 1.13s\n",
      "Warning: LLM returned None for query 'Where did Joe Biden graduated?'—handling as empty.\n",
      "Processed: Where did Joe Biden graduated? | Plain: 2.31s | RAG: 2.22s\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "query",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "plain_answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rag_answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "plain_latency",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rag_latency",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "b0b9a1c6-7127-4f7d-a9eb-a68cf9055fc8",
       "rows": [
        [
         "0",
         "What happened to Bidens' wife?",
         "There’s no credible evidence that anything bad has happened to Jill Biden. She is alive and continues to work as a teacher, author, and First Lady. If you’ve come across a story claiming otherwise, it’s a rumor or misinformation—Jill Biden hasn't been the victim of any violent incident or death.",
         "I don't have info on that.\n\nSources: 1.txt",
         "2.85200834274292",
         "1.666628122329712"
        ],
        [
         "1",
         "Where was Joe Biden born?",
         "Joe Biden was born in Scranton, **Pennsylvania**, USA.",
         "Joe Biden was born in Scranton, Pennsylvania.\n\nSources: 1.txt",
         "1.0012609958648682",
         "1.1292359828948975"
        ],
        [
         "2",
         "Where did Joe Biden graduated?",
         "",
         "Joe Biden graduated from the University of Delaware in 1965 and from Syracuse University College of Law (now part of Syracuse University) in 1968.\n\nSources: 1.txt",
         "2.3116705417633057",
         "2.221756935119629"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>plain_answer</th>\n",
       "      <th>rag_answer</th>\n",
       "      <th>plain_latency</th>\n",
       "      <th>rag_latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What happened to Bidens' wife?</td>\n",
       "      <td>There’s no credible evidence that anything bad...</td>\n",
       "      <td>I don't have info on that.\\n\\nSources: 1.txt</td>\n",
       "      <td>2.852008</td>\n",
       "      <td>1.666628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where was Joe Biden born?</td>\n",
       "      <td>Joe Biden was born in Scranton, **Pennsylvania...</td>\n",
       "      <td>Joe Biden was born in Scranton, Pennsylvania.\\...</td>\n",
       "      <td>1.001261</td>\n",
       "      <td>1.129236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where did Joe Biden graduated?</td>\n",
       "      <td></td>\n",
       "      <td>Joe Biden graduated from the University of Del...</td>\n",
       "      <td>2.311671</td>\n",
       "      <td>2.221757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            query  \\\n",
       "0  What happened to Bidens' wife?   \n",
       "1       Where was Joe Biden born?   \n",
       "2  Where did Joe Biden graduated?   \n",
       "\n",
       "                                        plain_answer  \\\n",
       "0  There’s no credible evidence that anything bad...   \n",
       "1  Joe Biden was born in Scranton, **Pennsylvania...   \n",
       "2                                                      \n",
       "\n",
       "                                          rag_answer  plain_latency  \\\n",
       "0       I don't have info on that.\\n\\nSources: 1.txt       2.852008   \n",
       "1  Joe Biden was born in Scranton, Pennsylvania.\\...       1.001261   \n",
       "2  Joe Biden graduated from the University of Del...       2.311671   \n",
       "\n",
       "   rag_latency  \n",
       "0     1.666628  \n",
       "1     1.129236  \n",
       "2     2.221757  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST\n",
    "# Test with 3 quick queries \n",
    "test_queries = [\"What happened to Bidens' wife?\", \"Where was Joe Biden born?\", \"Where did Joe Biden graduated?\"]\n",
    "df = compare_ragged_outputs(test_queries, embed_model, index, all_chunks, max_length=100)\n",
    "df\n",
    "#RAG shows no hallucination and cite its sources. Plain LLM hallucinates more. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
