{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b193a7",
   "metadata": {},
   "source": [
    "Ingestion - Input data and chunk them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7d5815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\\1.txt...\n",
      "Added 238 chunks from 1.txt\n",
      "Loading data\\2.txt...\n",
      "Added 11 chunks from 2.txt\n",
      "Loading data\\3.txt...\n",
      "Added 60 chunks from 3.txt\n",
      "Loading data\\4.txt...\n",
      "Added 63 chunks from 4.txt\n",
      "Loading data\\5.txt...\n",
      "Added 13 chunks from 5.txt\n",
      "Loading data\\6.txt...\n",
      "Added 6 chunks from 6.txt\n",
      "Loading data\\7.txt...\n",
      "Added 8 chunks from 7.txt\n",
      "Loading data\\8.txt...\n",
      "Added 9 chunks from 8.txt\n",
      "Loading data\\9.txt...\n",
      "Added 5 chunks from 9.txt\n",
      "Loading data\\10.txt...\n",
      "Added 13 chunks from 10.txt\n",
      "Total chunks created: 426\n",
      "Chunks saved to chunks.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import pickle  # For saving chunks if needed\n",
    "\n",
    "# Path to your data folder\n",
    "data_dir = \"data\"  # Assumes /data in your repo root\n",
    "\n",
    "# List to hold all chunks\n",
    "all_chunks = []\n",
    "\n",
    "# Loop over files 1.txt to 15.txt (skips missing ones)\n",
    "for i in range(1, 11):  # 1 to 15\n",
    "    file_path = os.path.join(data_dir, f\"{i}.txt\")\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading {file_path}...\")\n",
    "        loader = TextLoader(file_path, encoding=\"utf-8\")  # Handles standard text\n",
    "        docs = loader.load()\n",
    "        text = \" \".join([doc.page_content for doc in docs])  # Just the text\n",
    "\n",
    "        # Clean: Remove extra whitespace, newlines (basic)\n",
    "        text = ' '.join(text.split())  # Collapses multiples\n",
    "        text = text.replace('\\n', ' ')  # Flatten newlines if any\n",
    "\n",
    "        # Chunk this file's text\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,  # ~500 chars/tokens\n",
    "            chunk_overlap=50  # Overlap for context continuity\n",
    "\n",
    "        )\n",
    "        file_chunks = splitter.split_text(text)\n",
    "        for chunk in file_chunks:\n",
    "            all_chunks.append({'text': chunk, 'source': f\"{i}.txt\"})\n",
    "        print(f\"Added {len(file_chunks)} chunks from {i}.txt\")\n",
    "    else:\n",
    "        print(f\"Skipping {file_path} (not found)\")\n",
    "\n",
    "# Final count\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "\n",
    "\n",
    "with open(\"chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_chunks, f)\n",
    "print(\"Chunks saved to chunks.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1e911",
   "metadata": {},
   "source": [
    "Loading LLM model: Qwen/Qwen2.5-7B-Instruct-1M - better for long context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7118f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline, set_seed\n",
    "\n",
    "# generator = pipeline('question-answering', model='gpt2')\n",
    "# set_seed(2025)\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "client = InferenceClient(\"Qwen/Qwen2.5-7B-Instruct-1M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f49a9b",
   "metadata": {},
   "source": [
    "Load processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeefab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 426 chunks\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"chunks.pkl\", \"rb\") as f:\n",
    "    all_chunks = pickle.load(f)\n",
    "print(f\"Loaded {len(all_chunks)} chunks\")  # e.g., 30 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b9520",
   "metadata": {},
   "source": [
    "Embed the chunks above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7cb03ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (426, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')  # From Day 1\n",
    "embeddings = embed_model.encode(all_chunks)  # List of arrays -> one big array\n",
    "embeddings = np.array(embeddings).astype('float32')  # FAISS needs this\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")  # e.g., (30, 384) â€“ chunks x dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8feb1",
   "metadata": {},
   "source": [
    "Using Faiss for fast vector database searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e2a4863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built with 426 vectors\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "dimension = embeddings.shape[1]  # e.g., 384\n",
    "index = faiss.IndexFlatL2(dimension)  # Basic flat index (exact search, good for small data)\n",
    "index.add(embeddings)  # Train/add your vectors #type: Ignore\n",
    "print(f\"Index built with {index.ntotal} vectors\")  # Matches chunk count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc42638",
   "metadata": {},
   "source": [
    "Chat completion and comparision between two pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd8a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting response\n",
    "import pandas as pd\n",
    "import time\n",
    "from pipelines import *  # Assuming this imports retrieve_chunks, etc.\n",
    "import re\n",
    "from huggingface_hub import InferenceClient  # Add if not in pipelines\n",
    "\n",
    "# Assume client is defined, e.g.\n",
    "# client = InferenceClient(model=\"openai/gpt-oss-20b\")\n",
    "\n",
    "# Define augment_prompt_no_query based on the idea\n",
    "def augment_prompt_no_query(retrieved_chunks):\n",
    "    context_parts = []\n",
    "    for chunk_tuple in retrieved_chunks:\n",
    "        chunk_dict = chunk_tuple[0]  # {'text':, 'source':}\n",
    "        source = chunk_dict['source']\n",
    "        text = chunk_dict['text']\n",
    "        context_parts.append(f\"[Source: {source}]\\n{text}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    prompt = f\"\"\"Use the following context to answer the question factually. If the context doesn't cover it, say \"I don't have info on that.\"\n",
    "When referencing information from the context, include the [Source: x.txt] inline in your answer where it's used.\n",
    "    \n",
    "Context: {context}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def compare_ragged_outputs(queries, embed_model, index, all_chunks, max_length=300):\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Plain LLM\n",
    "        plain_messages = [{\"role\": \"user\", \"content\": query}]\n",
    "        plain_response = client.chat_completion(plain_messages, max_tokens=300)\n",
    "        plain_output = plain_response.choices[0].message.content  # Extract content\n",
    "        plain_time = time.time() - start_time\n",
    "        \n",
    "        # RAG\n",
    "        rag_start = time.time()\n",
    "        retrieved = retrieve_chunks(query, embed_model, index, all_chunks)\n",
    "        rag_messages = [{\"role\": \"system\", \"content\": augment_prompt_no_query(retrieved)}, {\"role\": \"user\", \"content\": query}]  # System first\n",
    "        rag_response = client.chat_completion(rag_messages, max_tokens=300)\n",
    "        ragged_output = rag_response.choices[0].message.content\n",
    "        used_sources = \"\"\n",
    "        if ragged_output:\n",
    "            used_sources = set(re.findall(pattern=r'\\[Source: \\(\\d+\\.txt\\)\\]', string=ragged_output))\n",
    "        \n",
    "        # If no citations parsed, fall back to all retrieved sources\n",
    "        if not used_sources:\n",
    "            used_sources = set(chunk_tuple[0]['source'] for chunk_tuple in retrieved)\n",
    "        \n",
    "        # Sort and format\n",
    "        sources_list = sorted(used_sources)\n",
    "        sources_str = \"Sources: \" + \", \".join(sources_list) if used_sources else \"No sources used.\"\n",
    "        \n",
    "        # Append to ragged_output\n",
    "        full_output = ragged_output + \"\\n\\n\" + sources_str\n",
    "\n",
    "        rag_time = time.time() - rag_start\n",
    "        \n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'plain_answer': plain_output,\n",
    "            'rag_answer': full_output,\n",
    "            'plain_latency': plain_time,\n",
    "            'rag_latency': rag_time\n",
    "        })\n",
    "        print(f\"Processed: {query} | Plain: {plain_time:.2f}s | RAG: {rag_time:.2f}s\")\n",
    "    \n",
    "    # Save to CSV if you want\n",
    "    df = pd.DataFrame(results)\n",
    "    # df.to_csv('comparison_results.csv', index=False)\n",
    "    # print(\"Results saved to comparison_results.csv\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b93ac757",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "504 Server Error: Gateway Time-out for url: https://router.huggingface.co/featherless-ai/v1/chat/completions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:407\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 504 Server Error: Gateway Time-out for url: https://router.huggingface.co/featherless-ai/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#TEST\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Test with 3 quick queries first\u001b[39;00m\n\u001b[0;32m      3\u001b[0m test_queries \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me what happened to Joe Biden\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms , the 46th president of the USA, wife\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me Where was Joe Biden, the 46th president of the USA, born\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me where did Joe Biden, the 46th president of the USA, graduated\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_ragged_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m df\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#RAG shows no hallucination and cite its sources. Plain LLM hallucinates more. \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 34\u001b[0m, in \u001b[0;36mcompare_ragged_outputs\u001b[1;34m(queries, embed_model, index, all_chunks, max_length)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Plain LLM\u001b[39;00m\n\u001b[0;32m     33\u001b[0m plain_messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: query}]\n\u001b[1;32m---> 34\u001b[0m plain_response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplain_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m plain_output \u001b[38;5;241m=\u001b[39m plain_response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent  \u001b[38;5;66;03m# Extract content\u001b[39;00m\n\u001b[0;32m     36\u001b[0m plain_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:919\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[1;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[0;32m    891\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload_model,\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[0;32m    911\u001b[0m }\n\u001b[0;32m    912\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mprepare_request(\n\u001b[0;32m    913\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    914\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    917\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[0;32m    918\u001b[0m )\n\u001b[1;32m--> 919\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:275\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[1;34m(self, request_parameters, stream)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 275\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:480\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 480\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 504 Server Error: Gateway Time-out for url: https://router.huggingface.co/featherless-ai/v1/chat/completions"
     ]
    }
   ],
   "source": [
    "#TEST\n",
    "# Test with 3 quick queries first\n",
    "test_queries = [\"Tell me what happened to Joe Biden's , the 46th president of the USA, wife\", \"Tell me Where was Joe Biden, the 46th president of the USA, born\", \"Tell me where did Joe Biden, the 46th president of the USA, graduated\"]\n",
    "df = compare_ragged_outputs(test_queries, embed_model, index, all_chunks, max_length=100)\n",
    "df\n",
    "#RAG shows no hallucination and cite its sources. Plain LLM hallucinates more. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
