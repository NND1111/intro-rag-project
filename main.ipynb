{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b193a7",
   "metadata": {},
   "source": [
    "Ingestion - Input data and chunk them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7d5815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\\1.txt...\n",
      "Added 238 chunks from 1.txt\n",
      "Loading data\\2.txt...\n",
      "Added 11 chunks from 2.txt\n",
      "Loading data\\3.txt...\n",
      "Added 60 chunks from 3.txt\n",
      "Loading data\\4.txt...\n",
      "Added 63 chunks from 4.txt\n",
      "Loading data\\5.txt...\n",
      "Added 13 chunks from 5.txt\n",
      "Loading data\\6.txt...\n",
      "Added 6 chunks from 6.txt\n",
      "Loading data\\7.txt...\n",
      "Added 8 chunks from 7.txt\n",
      "Loading data\\8.txt...\n",
      "Added 9 chunks from 8.txt\n",
      "Loading data\\9.txt...\n",
      "Added 5 chunks from 9.txt\n",
      "Loading data\\10.txt...\n",
      "Added 13 chunks from 10.txt\n",
      "Total chunks created: 426\n",
      "Chunks saved to chunks.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import pickle  # For saving chunks if needed\n",
    "\n",
    "# Path to your data folder\n",
    "data_dir = \"data\"  # Assumes /data in your repo root\n",
    "\n",
    "# List to hold all chunks\n",
    "all_chunks = []\n",
    "\n",
    "# Loop over files 1.txt to 15.txt (skips missing ones)\n",
    "for i in range(1, 11):  # 1 to 15\n",
    "    file_path = os.path.join(data_dir, f\"{i}.txt\")\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading {file_path}...\")\n",
    "        loader = TextLoader(file_path, encoding=\"utf-8\")  # Handles standard text\n",
    "        docs = loader.load()\n",
    "        text = \" \".join([doc.page_content for doc in docs])  # Just the text\n",
    "\n",
    "        # Clean: Remove extra whitespace, newlines (basic)\n",
    "        text = ' '.join(text.split())  # Collapses multiples\n",
    "        text = text.replace('\\n', ' ')  # Flatten newlines if any\n",
    "\n",
    "        # Chunk this file's text\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,  # ~500 chars/tokens\n",
    "            chunk_overlap=50  # Overlap for context continuity\n",
    "\n",
    "        )\n",
    "        file_chunks = splitter.split_text(text)\n",
    "        for chunk in file_chunks:\n",
    "            all_chunks.append({'text': chunk, 'source': f\"{i}.txt\"})\n",
    "        print(f\"Added {len(file_chunks)} chunks from {i}.txt\")\n",
    "    else:\n",
    "        print(f\"Skipping {file_path} (not found)\")\n",
    "\n",
    "# Final count\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "\n",
    "\n",
    "with open(\"chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_chunks, f)\n",
    "print(\"Chunks saved to chunks.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1e911",
   "metadata": {},
   "source": [
    "Loading LLM model: GPT 2 - training knowledge cutoff - 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7118f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline, set_seed\n",
    "\n",
    "# generator = pipeline('question-answering', model='gpt2')\n",
    "# set_seed(2025)\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "client = InferenceClient(\"Qwen/Qwen2.5-7B-Instruct-1M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f49a9b",
   "metadata": {},
   "source": [
    "Load processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeefab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 426 chunks\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"chunks.pkl\", \"rb\") as f:\n",
    "    all_chunks = pickle.load(f)\n",
    "print(f\"Loaded {len(all_chunks)} chunks\")  # e.g., 30 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b9520",
   "metadata": {},
   "source": [
    "Embed the chunks above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577f12c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7cb03ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (426, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')  # From Day 1\n",
    "embeddings = embed_model.encode(all_chunks)  # List of arrays -> one big array\n",
    "embeddings = np.array(embeddings).astype('float32')  # FAISS needs this\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")  # e.g., (30, 384) – chunks x dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8feb1",
   "metadata": {},
   "source": [
    "Using Faiss for fast vector database searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e2a4863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built with 426 vectors\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "dimension = embeddings.shape[1]  # e.g., 384\n",
    "index = faiss.IndexFlatL2(dimension)  # Basic flat index (exact search, good for small data)\n",
    "index.add(embeddings)  # Train/add your vectors #type: Ignore\n",
    "print(f\"Index built with {index.ntotal} vectors\")  # Matches chunk count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "577a84f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test code \n",
    "# dummy_query = embed_model.encode([\"AI bias in ethics\"])\n",
    "# distances, indices = index.search(dummy_query, k=3)  # Top 3 nearest\n",
    "# print(\"Top indices:\", indices)  # e.g., [5, 12, 3] – chunk IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "489984f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing retrival functionality\n",
    "# from retrieval import *\n",
    "# sample_query = \"With all the information given, The name of the 46th president of the United States is\"\n",
    "# results = retrieve_chunks(sample_query, embed_model, index, all_chunks)\n",
    "# for chunk, score in results:\n",
    "#     print(f\"Score: {score:.2f} | Chunk: {chunk[:100]}...\")  # Preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc42638",
   "metadata": {},
   "source": [
    "Defining RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa479ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afd8a85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: Tell me what happened to Joe Biden's , the 46th president of the USA, wife | Plain: 2.60s | RAG: 2.48s\n",
      "Processed: Tell me Where was Joe Biden, the 46th president of the USA, born | Plain: 1.42s | RAG: 1.86s\n",
      "Processed: Tell me where did Joe Biden, the 46th president of the USA, graduated | Plain: 1.78s | RAG: 2.35s\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "query",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "plain_answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rag_answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "plain_latency",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rag_latency",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "9c53a85e-c17e-423e-84a2-25287222e1e7",
       "rows": [
        [
         "0",
         "Tell me what happened to Joe Biden's , the 46th president of the USA, wife",
         "Jill Biden (born Jill Tracy Jacobs 1980",
         "I don't have info on that.\n\nSources: 1.txt",
         "2.5953269004821777",
         "2.4790496826171875"
        ],
        [
         "1",
         "Tell me Where was Joe Biden, the 46th president of the USA, born",
         "Joe Biden, the 46th President of the United States, was born in Scranton, Pennsylvania, USA.",
         "Joe Biden was born in Scranton, Pennsylvania. [Source: 1.txt]\n\nSources: 1.txt",
         "1.4154901504516602",
         "1.8580586910247803"
        ],
        [
         "2",
         "Tell me where did Joe Biden, the 46th president of the USA, graduated",
         "Joe Biden earned his bachelor’s degree from the **University of Delaware** (1965) and his law degree from **Syracuse University College of Law** (1968).",
         "Joe Biden earned his undergraduate degree from the University of Delaware in 1965 and completed his law degree at Syracuse University’s College of Law in 1968. [Source: 1.txt]\n\nSources: 1.txt",
         "1.7801542282104492",
         "2.345470428466797"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>plain_answer</th>\n",
       "      <th>rag_answer</th>\n",
       "      <th>plain_latency</th>\n",
       "      <th>rag_latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tell me what happened to Joe Biden's , the 46t...</td>\n",
       "      <td>Jill Biden (born Jill Tracy Jacobs 1980</td>\n",
       "      <td>I don't have info on that.\\n\\nSources: 1.txt</td>\n",
       "      <td>2.595327</td>\n",
       "      <td>2.479050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tell me Where was Joe Biden, the 46th presiden...</td>\n",
       "      <td>Joe Biden, the 46th President of the United St...</td>\n",
       "      <td>Joe Biden was born in Scranton, Pennsylvania. ...</td>\n",
       "      <td>1.415490</td>\n",
       "      <td>1.858059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tell me where did Joe Biden, the 46th presiden...</td>\n",
       "      <td>Joe Biden earned his bachelor’s degree from th...</td>\n",
       "      <td>Joe Biden earned his undergraduate degree from...</td>\n",
       "      <td>1.780154</td>\n",
       "      <td>2.345470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  Tell me what happened to Joe Biden's , the 46t...   \n",
       "1  Tell me Where was Joe Biden, the 46th presiden...   \n",
       "2  Tell me where did Joe Biden, the 46th presiden...   \n",
       "\n",
       "                                        plain_answer  \\\n",
       "0            Jill Biden (born Jill Tracy Jacobs 1980   \n",
       "1  Joe Biden, the 46th President of the United St...   \n",
       "2  Joe Biden earned his bachelor’s degree from th...   \n",
       "\n",
       "                                          rag_answer  plain_latency  \\\n",
       "0       I don't have info on that.\\n\\nSources: 1.txt       2.595327   \n",
       "1  Joe Biden was born in Scranton, Pennsylvania. ...       1.415490   \n",
       "2  Joe Biden earned his undergraduate degree from...       1.780154   \n",
       "\n",
       "   rag_latency  \n",
       "0     2.479050  \n",
       "1     1.858059  \n",
       "2     2.345470  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting response\n",
    "import pandas as pd\n",
    "import time\n",
    "from pipelines import *  # Assuming this imports retrieve_chunks, etc.\n",
    "import re\n",
    "from huggingface_hub import InferenceClient  # Add if not in pipelines\n",
    "\n",
    "# Assume client is defined, e.g.\n",
    "client = InferenceClient(model=\"openai/gpt-oss-20b\")\n",
    "\n",
    "# Define augment_prompt_no_query based on the idea\n",
    "def augment_prompt_no_query(retrieved_chunks):\n",
    "    context_parts = []\n",
    "    for chunk_tuple in retrieved_chunks:\n",
    "        chunk_dict = chunk_tuple[0]  # {'text':, 'source':}\n",
    "        source = chunk_dict['source']\n",
    "        text = chunk_dict['text']\n",
    "        context_parts.append(f\"[Source: {source}]\\n{text}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    prompt = f\"\"\"Use the following context to answer the question factually. If the context doesn't cover it, say \"I don't have info on that.\"\n",
    "When referencing information from the context, include the [Source: x.txt] inline in your answer where it's used.\n",
    "    \n",
    "Context: {context}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def compare_ragged_outputs(queries, embed_model, index, all_chunks, max_length=300):\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Plain LLM\n",
    "        plain_messages = [{\"role\": \"user\", \"content\": query}]\n",
    "        plain_response = client.chat_completion(plain_messages, max_tokens=300)\n",
    "        plain_output = plain_response.choices[0].message.content  # Extract content\n",
    "        plain_time = time.time() - start_time\n",
    "        \n",
    "        # RAG\n",
    "        rag_start = time.time()\n",
    "        retrieved = retrieve_chunks(query, embed_model, index, all_chunks)\n",
    "        rag_messages = [{\"role\": \"system\", \"content\": augment_prompt_no_query(retrieved)}, {\"role\": \"user\", \"content\": query}]  # System first\n",
    "        rag_response = client.chat_completion(rag_messages, max_tokens=300)\n",
    "        ragged_output = rag_response.choices[0].message.content\n",
    "        used_sources = \"\"\n",
    "        if ragged_output:\n",
    "            used_sources = set(re.findall(pattern=r'\\[Source: \\(\\d+\\.txt\\)\\]', string=ragged_output))\n",
    "        \n",
    "        # If no citations parsed, fall back to all retrieved sources\n",
    "        if not used_sources:\n",
    "            used_sources = set(chunk_tuple[0]['source'] for chunk_tuple in retrieved)\n",
    "        \n",
    "        # Sort and format\n",
    "        sources_list = sorted(used_sources)\n",
    "        sources_str = \"Sources: \" + \", \".join(sources_list) if used_sources else \"No sources used.\"\n",
    "        \n",
    "        # Append to ragged_output\n",
    "        full_output = ragged_output + \"\\n\\n\" + sources_str\n",
    "\n",
    "        rag_time = time.time() - rag_start\n",
    "        \n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'plain_answer': plain_output,\n",
    "            'rag_answer': full_output,\n",
    "            'plain_latency': plain_time,\n",
    "            'rag_latency': rag_time\n",
    "        })\n",
    "        print(f\"Processed: {query} | Plain: {plain_time:.2f}s | RAG: {rag_time:.2f}s\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    # df.to_csv('comparison_results.csv', index=False)\n",
    "    # print(\"Results saved to comparison_results.csv\")\n",
    "    return df\n",
    "\n",
    "# Test with 3 quick queries first\n",
    "test_queries = [\"Tell me what happened to Joe Biden's , the 46th president of the USA, wife\", \"Tell me Where was Joe Biden, the 46th president of the USA, born\", \"Tell me where did Joe Biden, the 46th president of the USA, graduated\"]\n",
    "df = compare_ragged_outputs(test_queries, embed_model, index, all_chunks, max_length=100)\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
